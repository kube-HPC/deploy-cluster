---
# Source: prometheus/templates/alertmanager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "alertmanager"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-alertmanager
data:
  alertmanager.yml: |-
    global:
      # slack_api_url: ''
  
    receivers:
      - name: default-receiver
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true
  
    route:
      group_wait: 10s
      #group_interval: 5m
      group_interval: 10s
      receiver: default-receiver
      repeat_interval: 3h
  

---
# Source: prometheus/templates/server-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server
data:
  alerts: |
    groups:
    ## Test alerts
    - name: test
      rules:
      - alert: MyTestAlert1
        expr: sum(hkube_worker_started_counter) > (sum(hkube_thres_mgr_test_gauge) or sum(absent(dummy)) * 3)
    #    for: 10m
        labels:
          severity: error
        annotations:
          summary: 'TEST started algs: {{ humanize $value }} (test threshold: {{ query "sum(hkube_thres_mgr_test_gauge) or sum(absent(dummy)) * 3" | first | value }})'

    ## Real Hkube alerts
    - name: hkube
      rules:
      ### THRESHOLD RECORD RULES (including defaults)
      # NOTE: 
      #	- thresholds are metrics from alerts-threshold-manager, if missing (e.g. not running) => default value is used.
      #	- threshold default value is accepted by 'or' operation with: 'sum(absent(dummy)) * <default> ' when <default> is any default value.
      #	- threshold values may be changed by PUT requests to alerts-threshold-manager (json format body)
      - record: GENERAL_RATE_GROW_THRES
        expr: (sum(hkube_thres_mgr_general_warn_rate_grow_gauge) or sum(absent(dummy))*0.2 )
      - record: PIPE_FAIL_RATE_FRACTION_THRES
        expr: (sum(hkube_thres_mgr_warn_pipeline_failure_rate_fraction_gauge) or sum(absent(dummy)) * 0.3)
      - record: ALG_FAIL_RATE_FRACTION_THRES
        expr: (sum(hkube_thres_mgr_warn_alg_failure_rate_fraction_gauge) or sum(absent(dummy)) * 0.3)
      - record: MIN_PIPE_TIME_IN_QUEUE_WARN
        expr: ((sum(hkube_thres_mgr_min_warn_pipeline_time_in_queue_gauge) or sum(absent(dummy)) * 1000 )/1000)
      - record: MIN_ALG_TIME_IN_QUEUE_WARN
        expr: ((sum(hkube_thres_mgr_min_warn_alg_time_in_queue_gauge) or sum(absent(dummy)) * 1000 )/1000)
      - record: CPU_USAGE_GROW_THRES
        expr: (sum(hkube_thres_mgr_warn_alg_cpu_usage_grow_gauge) or sum(absent(dummy)) * 0.4 )
      - record: MIN_CPU_USAGE_WARN
        expr: (sum(hkube_thres_mgr_warn_alg_cpu_usage_gauge) or sum(absent(dummy)) * 4 )
      - record: ALG_MEM_MB_THRES
        expr: (sum(hkube_thres_mgr_warn_alg_memory_usage_gauge) or sum(absent(dummy)) * 1000)


      ### OTHER RECORD RULES
      - record: pipeline_failures
        expr: hkube_api_server_pipelines_gross_counter{status='failed'}
      - record: pipe_recent_failure_rate
        expr: (sum(rate(pipeline_failures[5m])) * 60)
      - record: pipe_usual_failure_rate
        expr: (sum(rate(pipeline_failures[1d])) * 60)
      - record: alg_recent_failure_rate
        expr: (sum(rate(hkube_worker_failed_counter[5m])) * 60)
      - record: alg_usual_failure_rate
        expr: (sum(rate(hkube_worker_failed_counter[1d])) * 60)
      - record: alg_cpu_rate_5m
        expr: rate(container_cpu_usage_seconds_total{container_name="algorunner"}[5m])
      - record: alg_cpu_rate_1d
        expr: rate(container_cpu_usage_seconds_total{container_name="algorunner"}[1d])
      - record: alg_cpu_usage_5m
        expr: sum(max(kube_pod_labels{label_algorithm_name!~""}) by (pod) * on(pod) label_replace( sum by(pod_name) (alg_cpu_rate_5m), "pod", "$1", "pod_name", "(.+)"))
      - record: alg_cpu_usage_1d
        expr: sum(max(kube_pod_labels{label_algorithm_name!~""}) by (pod) * on(pod) label_replace( sum by(pod_name) (alg_cpu_rate_1d), "pod", "$1", "pod_name", "(.+)"))
      - record: alg_mem_usage_bytes
        expr: sum(max(kube_pod_labels{label_algorithm_name!~""}) by (pod)
                  * on(pod)
                  label_replace(
                      sum by(pod_name) (
                          container_memory_working_set_bytes
                      ), "pod", "$1", "pod_name", "(.+)")
                  )
      - record: pipe_recent_low_time_in_queue_sec
        expr: (0.001 * histogram_quantile(0.3, sum(rate(hkube_pipeline_driver_queue_time_in_queue_histogram_bucket[5m])) by (le)) )
      - record: pipe_usual_median_time_in_queue_sec
        expr: (0.001 * histogram_quantile(0.5, sum(rate(hkube_pipeline_driver_queue_time_in_queue_histogram_bucket[1d])) by (le)) )
      - record: alg_recent_low_time_in_queue_sec
        expr: (0.001 * histogram_quantile(0.3, sum(rate(hkube_algorithm_queue_time_in_queue_histogram_bucket[5m])) by (le)))
      - record: alg_usual_median_time_in_queue_sec
        expr: (0.001 * histogram_quantile(0.5, sum(rate(hkube_algorithm_queue_time_in_queue_histogram_bucket[1d])) by (le)))

      ###### ALERT RULES ######
 
      ### HighPipelineFailureRate: recent pipeline failure rate is higher than daily by at least by threshold percent
      - alert: HighPipelineFailureRate
        expr: pipe_recent_failure_rate > ( pipe_usual_failure_rate * (1 + GENERAL_RATE_GROW_THRES) )
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Recent 5 min Pipeline Failure Rate is higher than daily rate by at least {{ query "GENERAL_RATE_GROW_THRES * 100" | first | value }}%: {{$value | printf `%.2f`}} failures/min (daily: {{ query "pipe_usual_failure_rate" | first | value | printf `%.2f`}})'
 
      ### HighAlgorithmFailureRate: recent algorithm failure rate is higher than daily by at least by threshold percent
      - alert: HighAlgorithmFailureRate
        expr: alg_recent_failure_rate > ( alg_usual_failure_rate * ( 1 + GENERAL_RATE_GROW_THRES) )
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Recent 5 min Algorithm Failure Rate is higher than daily rate by at least {{ query "GENERAL_RATE_GROW_THRES * 100" | first | value }}%: {{$value | printf `%.2f`}} failures/min (daily: {{ query "alg_usual_failure_rate" | first | value | printf `%.2f`}})'
 
      ### HighAlgorithmFailurePercent: high percent of algorithm failures of all algorithms (if higher than threshold percent)
      - alert: HighAlgorithmFailurePercent
        expr: 100 * sum(rate(hkube_worker_failed_counter[5m])) / (sum(rate(hkube_worker_succeeded_counter[5m]) + rate(hkube_worker_failed_counter[5m]))) > (ALG_FAIL_RATE_FRACTION_THRES * 100)
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'High percent of Algorithm Failures: {{$value | printf `%.2f`}}% for at least 5 min (threshold: {{ query "ALG_FAIL_RATE_FRACTION_THRES * 100" | first | value }}%)'
 
       ### HighPipelineFailurePercent: high percent of pipeline failures of all pipelines (if higher than threshold percent)
      - alert: HighPipelineFailurePercent
        expr: 100 * sum(rate(hkube_api_server_pipelines_gross_counter{status='failed'}[5m])) / sum(rate(hkube_api_server_pipelines_gross_counter[5m])) > (PIPE_FAIL_RATE_FRACTION_THRES * 100)
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'High percent of Pipeline Failures: {{$value | printf `%.2f`}}% for at least 5 min (threshold: {{ query "PIPE_FAIL_RATE_FRACTION_THRES * 100" | first | value }}%)'
 
      ### LongAlgorithmTimeInQueue: recent low percentile algorithm time-in-queue is higher than daily median (if above some threshold)
      - alert: LongAlgorithmTimeInQueue
        expr: (alg_recent_low_time_in_queue_sec > alg_usual_median_time_in_queue_sec) and (alg_recent_low_time_in_queue_sec > MIN_ALG_TIME_IN_QUEUE_WARN)
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Recent 5 min 30% percentile Algorithm Time-in-Queue is higher than daily median: {{ query "alg_recent_low_time_in_queue_sec" | first | value | humanizeDuration}} (median: {{ query "alg_usual_median_time_in_queue_sec" | first | value | humanizeDuration }})'
 
      ### LongPipelineTimeInQueue: recent low percentile pipeline time-in-queue is higher than daily median (if above some threshold)
      - alert: LongPipelineTimeInQueue
        expr: (pipe_recent_low_time_in_queue_sec > pipe_usual_median_time_in_queue_sec) and (pipe_recent_low_time_in_queue_sec > MIN_PIPE_TIME_IN_QUEUE_WARN)
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Recent 5 min 30% percentile Pipeline Time-in-Queue is higher than daily median: {{ query "pipe_recent_low_time_in_queue_sec" | first | value | humanizeDuration}} (median: {{ query "pipe_usual_median_time_in_queue_sec" | first | value | humanizeDuration }})'
 
      ### HighAlgorithmMemoryUsage: total algorithms memory usage is higher than threshold
      - alert: HighAlgorithmMemoryUsage
        expr: alg_mem_usage_bytes / (1024 * 1024) > ALG_MEM_MB_THRES
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'High Algorithms Total Memory Usage: {{$value | printf "%.0f"}} MB for at least 5 min (threshold: {{ query "ALG_MEM_MB_THRES" | first | value }})'

      ### HighAlgorithmCpuUsage: recent CPU usage is higher than daily by at least threshold percent (if above some threshold)
      - alert: HighAlgorithmCpuUsage
        expr: (alg_cpu_usage_5m > (alg_cpu_usage_1d * (1 + CPU_USAGE_GROW_THRES))) and (alg_cpu_usage_5m > MIN_CPU_USAGE_WARN)
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: 'Recent 5 min Algorithms CPU Usage {{query "alg_cpu_usage_5m" | first | value | printf "%.2f"}} cores is higher than daily by more than {{ query "CPU_USAGE_GROW_THRES * 100" | first | value }}%'

    
  prometheus.yml: |
    rule_files:
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - job_name: kubernetes-apiservers
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-nodes
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/${1}/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-nodes-cadvisor
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/${1}:4194/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+)(?::\d+);(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: prometheus-pushgateway
      honor_labels: true
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-node-exporter
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      scheme: https
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - source_labels: [__address__]
        regex: ^(.*):\d+$
        target_label: __address__
        replacement: $1:9100
      - target_label: __scheme__
        replacement: http
      # Host name
      - source_labels: [__meta_kubernetes_node_name]
        target_label: node
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+):(?:\d+);(\d+)
        replacement: ${1}:${2}
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: monitoring
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: prometheus
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_component]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex:
          action: drop
  rules: |
    {}
    
---
# # Source: prometheus/templates/alertmanager-pvc.yaml
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   labels:
#     app: prometheus
#     chart: prometheus-5.0.2
#     component: "alertmanager"
#     heritage: Tiller
#     release: monitoring
#   name: monitoring-prometheus-alertmanager
# spec:
#   accessModes:
#     - ReadWriteOnce
    
#   resources:
#     requests:
#       storage: "2Gi"
# ---
# # Source: prometheus/templates/server-pvc.yaml
# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   labels:
#     app: prometheus
#     chart: prometheus-5.0.2
#     component: "server"
#     heritage: Tiller
#     release: monitoring
#   name: monitoring-prometheus-server
# spec:
#   accessModes:
#     - ReadWriteOnce
    
#   resources:
#     requests:
#       storage: "8Gi"
# ---
# Source: prometheus/templates/alertmanager-serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "alertmanager"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-alertmanager

---
# Source: prometheus/templates/kube-state-metrics-serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "kube-state-metrics"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-kube-state-metrics

---
# Source: prometheus/templates/node-exporter-serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "node-exporter"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-node-exporter

---
# Source: prometheus/templates/server-serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server

---
# Source: prometheus/templates/kube-state-metrics-clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "kube-state-metrics"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-kube-state-metrics
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
      - nodes
      - persistentvolumeclaims
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
    verbs:
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - list
      - watch

---
# Source: prometheus/templates/server-clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get

---
# Source: prometheus/templates/alertmanager-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "alertmanager"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-alertmanager
subjects:
  - kind: ServiceAccount
    name: monitoring-prometheus-alertmanager
    namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin

---
# Source: prometheus/templates/kube-state-metrics-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "kube-state-metrics"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: monitoring-prometheus-kube-state-metrics
    namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: monitoring-prometheus-kube-state-metrics

---
# Source: prometheus/templates/node-exporter-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "node-exporter"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-node-exporter
subjects:
  - kind: ServiceAccount
    name: monitoring-prometheus-node-exporter
    namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin

---
# Source: prometheus/templates/server-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server
subjects:
  - kind: ServiceAccount
    name: monitoring-prometheus-server
    namespace: monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: monitoring-prometheus-server

---
# Source: prometheus/templates/alertmanager-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "alertmanager"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-alertmanager
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9093
  selector:
    app: prometheus
    component: "alertmanager"
    release: monitoring
  type: "ClusterIP"

---
# Source: prometheus/templates/kube-state-metrics-svc.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "kube-state-metrics"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-kube-state-metrics
spec:
  clusterIP: None
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: prometheus
    component: "kube-state-metrics"
    release: monitoring
  type: "ClusterIP"

---
# Source: prometheus/templates/node-exporter-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    # prometheus.io/scrape: "true"
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "node-exporter"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-node-exporter
spec:
  clusterIP: None
  ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
  selector:
    app: prometheus
    component: "node-exporter"
    release: monitoring
  type: "ClusterIP"
---
# Source: prometheus/templates/pushgateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
    
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "pushgateway"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-pushgateway
spec:
  ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
  selector:
    app: prometheus
    component: "pushgateway"
    release: monitoring
  type: "ClusterIP"

---
# Source: prometheus/templates/server-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
      nodePort: 30909
  selector:
    app: prometheus
    component: "server"
    release: monitoring
  type: "NodePort"

---
# Source: prometheus/templates/node-exporter-daemonset.yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "node-exporter"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-node-exporter
spec:
  updateStrategy:
    type: OnDelete
    
  template:
    metadata:
      labels:
        app: prometheus
        component: "node-exporter"
        release: monitoring
    spec:
      serviceAccountName: monitoring-prometheus-node-exporter
      containers:
        - name: prometheus-node-exporter
          image: "prom/node-exporter:v0.15.2"
          imagePullPolicy: "IfNotPresent"
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
          ports:
            - name: metrics
              containerPort: 9100
              hostPort: 9100
          resources:
            {}
            
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
      hostNetwork: true
      hostPID: true
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
---
# Source: prometheus/templates/alertmanager-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "alertmanager"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-alertmanager
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus
        component: "alertmanager"
        release: monitoring
    spec:
      serviceAccountName: monitoring-prometheus-alertmanager
      containers:
        - name: prometheus-alertmanager
          image: "prom/alertmanager:v0.13.0"
          imagePullPolicy: "IfNotPresent"
          env:
          args:
            - --config.file=/etc/config/alertmanager.yml
            - --storage.path=/data
            - --web.external-url=/

          ports:
            - containerPort: 9093
          readinessProbe:
            httpGet:
              path: /#/status
              port: 9093
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: "/data"
              subPath: ""

        - name: prometheus-alertmanager-configmap-reload
          image: "jimmidyson/configmap-reload:v0.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://localhost:9093/-/reload
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: monitoring-prometheus-alertmanager
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-alertmanager-nfs-pvc

---
# Source: prometheus/templates/kube-state-metrics-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "kube-state-metrics"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-kube-state-metrics
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus
        component: "kube-state-metrics"
        release: monitoring
    spec:
      serviceAccountName: monitoring-prometheus-kube-state-metrics
      containers:
        - name: prometheus-kube-state-metrics
          image: "k8s.gcr.io/kube-state-metrics:v1.1.0"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: metrics
              containerPort: 8080
          resources:
            {}
            

---
# Source: prometheus/templates/pushgateway-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "pushgateway"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-pushgateway
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus
        component: "pushgateway"
        release: monitoring
    spec:
      containers:
        - name: prometheus-pushgateway
          image: "prom/pushgateway:v0.4.0"
          imagePullPolicy: "IfNotPresent"
          args:
          ports:
            - containerPort: 9091
          readinessProbe:
            httpGet:
              path: /#/status
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          resources:
            {}
            

---
# Source: prometheus/templates/server-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: prometheus
    chart: prometheus-5.0.2
    component: "server"
    heritage: Tiller
    release: monitoring
  name: monitoring-prometheus-server
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus
        component: "server"
        release: monitoring
    spec:
      serviceAccountName: monitoring-prometheus-server
      initContainers:
      - name: "init-chown-data"
        image: "busybox"
        # 65534 is the nobody user that prometheus uses.
        command: ["chown", "-R", "65534:65534", "/data"]
        volumeMounts:
        - name: storage-volume
          mountPath: /data
          subPath: ""
      containers:
        - name: prometheus-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://localhost:9090/-/reload
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "prom/prometheus:v2.1.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
            
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: monitoring-prometheus-server
        - name: storage-volume
          persistentVolumeClaim:
            claimName: prometheus-nfs-pvc

---
# Source: prometheus/templates/alertmanager-ingress.yaml

---
# Source: prometheus/templates/alertmanager-networkpolicy.yaml


---
# Source: prometheus/templates/kube-state-metrics-networkpolicy.yaml


---
# Source: prometheus/templates/pushgateway-ingress.yaml

---
# Source: prometheus/templates/server-ingress.yaml

---
# Source: prometheus/templates/server-networkpolicy.yaml


